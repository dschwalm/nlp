{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a35f62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import logging as log\n",
    "log.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=log.INFO)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import text_normalizer as tn\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn import tree\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import lightgbm as lgbm\n",
    "\n",
    "import shap\n",
    "\n",
    "import fasttext\n",
    "\n",
    "import gensim\n",
    "from gensim import utils as gensim_utils\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "pd.set_option('display.max_rows',1000)\n",
    "pd.set_option('display.max_columns',1000)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(37)\n",
    "random.seed(17)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords = stopwords + ['los', 'must', 'may', 'could','jim','would','without','also','thus','however','ben']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b1ef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_closestwords_tsnescatterplot(model, dim, words, topn=10):\n",
    "    \n",
    "    arr = np.empty((0,dim), dtype='f')\n",
    "    word_labels = words\n",
    "\n",
    "    # get close words\n",
    "    #close_words = [model.similar_by_word(word) for word in words]\n",
    "    \n",
    "    # add the vector for each of the closest words to the array\n",
    "    close_words=[]\n",
    "    for word in words:\n",
    "        arr = np.append(arr, np.array([model[word]]), axis=0)\n",
    "        close_words +=model.similar_by_word(word, topn=topn)\n",
    "        \n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "        \n",
    "    # find tsne coords for 2 dimensions\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    #np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    # display scatter plot\n",
    "    plt.scatter(x_coords, y_coords)\n",
    "\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
    "    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
    "    plt.show()\n",
    "    \n",
    "def tsne_plot(model, words):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    #for word in model.wv.vocab:\n",
    "    for word in words:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(14, 10)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722b1bc7",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fae264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MovieSummaries dataset. Source: http://www.cs.cmu.edu/~ark/personas/\n",
    "df_meta = pd.read_csv('movie_genre_prediction/movie.metadata.tsv', sep='\\t')\n",
    "df_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11559ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = pd.read_csv('movie_genre_prediction/plot_summaries.txt', sep='\\t')\n",
    "#df_plot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80f8cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df_plot, df_meta,on='movie_id',how='left')\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2222a34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['x1','title','date','x2','x3','lang','country'],axis=1,inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5cfd6415",
   "metadata": {},
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ed6e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['plot'] = df['plot'].astype(str)\n",
    "df['tags'] = df['tags'].astype(str)\n",
    "df['tags'] = df['tags'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6987cae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sci'] = ''\n",
    "df['sci'] = df['tags'].apply(lambda x : 1 if 'science fiction' in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0590763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sci = df[df['sci'] == 1]\n",
    "df_sci.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18015649",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_sci = df[df['sci'] == 0][:2500]\n",
    "df_non_sci.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_sci,df_non_sci])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854dad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['sci'].value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "987f9b5b",
   "metadata": {},
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5181fdb2",
   "metadata": {},
   "source": [
    "# document_string = \"\"\"In the final 23 days of L's life, he meets one final case involving a bioterrorist group that aims to wipe out much of humanity with a virus. The virus has an infection rate that has one hundred times the infection rate of the Ebola virus. He takes a boy he names Near, the sole survivor of its use in a village in Thailand, and an elementary school student named Maki Nikaido under his wing. Dr. Nikaido later received a sample of the deadly virus which destroyed that village in Thailand. His assistant, Dr. Kimiko Kujo, reveals herself to be the leader of the organization that created the virus. Dr. Nikaido, who has created an antidote to that virus, refuses to give it her. He destroys the antidote and injects himself with the virus. She later kills him, and she is convinced that his daughter Maki has the antidote formula. Under the pursuit of Dr. Kimiko Kujo and her assistants, Maki runs and escapes. She eventually found L's headquarters. However, the group manages to track Maki down, forcing L, accompanied by Maki and Near, to run away with a high-tech crepe truck. They also received the help of FBI agent Hideaki Suruga during the escape. They escape to Nikaido's research partner's lab, because they needed his help to recreate the antidote. Using Near, L manages to acquire the antidote just as the terrorists are about to take an infected Maki to the US to spread the virus. L stops the plane and gives all the infected passengers, including the terrorist, the antidote. Maki then tries to kill Kujo for revenge, but L stops her, she goes to the hospital and wakes up with her stuffed bear next to her and a recording from L telling her to have a good day tomorrow. The film concludes with L leaving Near and giving him Near's \"real name\". \"\"\"\n",
    "    \n",
    "from spacy.tokens import Span\n",
    "from spacy import Language\n",
    "import dateparser\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "@Language.component(\"my_pipeline_component\")\n",
    "def expand_person_entities(doc):\n",
    "    new_ents = []\n",
    "    for ent in doc.ents:\n",
    "        # Only check for title if it's a person and not the first token\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            if ent.start != 0:\n",
    "                # if person preceded by title, include title in entity\n",
    "                prev_token = doc[ent.start - 1]\n",
    "                if prev_token.text in (\"Dr\", \"Dr.\", \"Mr\", \"Mr.\", \"Ms\", \"Ms.\"):\n",
    "                    new_ent = Span(doc, ent.start - 1, ent.end, label=ent.label)\n",
    "                    new_ents.append(new_ent)\n",
    "                else:\n",
    "                    # if entity can be parsed as a date, it's not a person\n",
    "                    if dateparser.parse(ent.text) is None:\n",
    "                        new_ents.append(ent) \n",
    "        else:\n",
    "            new_ents.append(ent)\n",
    "    doc.ents = new_ents\n",
    "    return doc\n",
    "\n",
    "\n",
    "nlp.add_pipe(\"my_pipeline_component\", after='ner')\n",
    "\n",
    "for index in tqdm(df_train.index):\n",
    "    person_names = set()\n",
    "    doc = nlp(df_train.at[index, 'plot'])\n",
    "\n",
    "    for person_name, _ in [(ent.text, ent.label_) for ent in doc.ents if ent.label_=='PERSON']:\n",
    "        person_names.add(person_name)\n",
    "        \n",
    "    for name in person_names:\n",
    "        #df_train.iloc[index]['plot'] = df_train.iloc[index]['plot'].replace(name,'',df_train.iloc[index]['plot'])\n",
    "        df_train.at[index, 'plot'] = df_train.at[index, 'plot'].replace(name,'')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d233cd9e",
   "metadata": {},
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc53d034",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['cleaned_plot'] = tn.normalize_corpus(corpus=df_train['plot'],stopwords=stopwords)\n",
    "df_train.drop(['plot','tags'],axis=1,inplace=True)\n",
    "df_train.to_csv('cleaned_plots.csv',index=False)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba5427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['cleaned_plot'] = df_train['plot']\n",
    "df_train.drop(['plot','tags'],axis=1,inplace=True)\n",
    "df_train.to_csv('cleaned_plots_original.csv',index=False)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d5f5e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a75b3102",
   "metadata": {},
   "source": [
    "### TF IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6fffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('movie_genre_prediction/cleaned_plots.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f594ffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 700\n",
    "min_df = 10\n",
    "max_df = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315745a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=31, shuffle=True, stratify=df['sci'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e9b2eb44",
   "metadata": {},
   "source": [
    "count_vect = CountVectorizer(stop_words=stopwords,analyzer='word',token_pattern='[^\\W\\d_]{2,}',\\\n",
    "                             ngram_range=(1,3), max_features=max_features, min_df=min_df,max_df=max_df,strip_accents='unicode')\n",
    "X_train_counts = count_vect.fit_transform(texts)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "weights = np.asarray(X_train_tfidf.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': count_vect.get_feature_names(), 'weight': weights})\n",
    "weights_df = weights_df.sort_values(ascending=False, by=['weight'])\n",
    "weights_df.head(100)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d664a2e",
   "metadata": {},
   "source": [
    "weights_df.tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10ffc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=max_features, min_df=min_df,max_df=max_df,stop_words=stopwords,analyzer='word',\\\n",
    "                            token_pattern='[^\\W\\d_]{2,}', ngram_range=(1,3),strip_accents='unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3f7b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = df_train['cleaned_plot'].tolist()\n",
    "\n",
    "tfidf.fit(train_texts)\n",
    "\n",
    "tfidf_train = np.array(tfidf.transform(df_train['cleaned_plot']).todense(), dtype=np.float16)\n",
    "\n",
    "tfidf_feature_names = { v:k for k,v in tfidf.vocabulary_.items() }\n",
    "\n",
    "for i in range(max_features):\n",
    "    df_train['tfidf_' + tfidf_feature_names[i]] = tfidf_train[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84350236",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test = np.array(tfidf.transform(df_test['cleaned_plot']).todense(), dtype=np.float16)\n",
    "\n",
    "for i in range(max_features):\n",
    "    df_test['tfidf_' + tfidf_feature_names[i]] = tfidf_test[:, i]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b5a07d0",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bf7b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(['movie_id','sci','cleaned_plot'], axis=1,errors='ignore')\n",
    "y_train = df_train['sci']\n",
    "X_test = df_test.drop(['movie_id','sci','cleaned_plot'], axis=1,errors='ignore')\n",
    "y_test = df_test['sci']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4911bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "knn = KNeighborsClassifier()\n",
    "d_tree = DecisionTreeClassifier()\n",
    "forest = RandomForestClassifier()\n",
    "svm = LinearSVC()\n",
    "lgm = lgbm.LGBMClassifier()\n",
    "\n",
    "for model in [svm, knn,d_tree,lr,lgm,forest]:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print ('%s accuracy score: %f' % (model.__class__.__name__, model.score(X_test, y_test)))\n",
    "    #print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2b9007ad",
   "metadata": {},
   "source": [
    "np.where(y_test != y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf232e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = X_test.copy()\n",
    "df_out['truth'] = y_test\n",
    "df_out.reset_index(inplace=True)\n",
    "df_out['predicted'] = y_pred\n",
    "df_misclassified = df_out[df_out['truth'] != df_out['predicted']][['index','truth','predicted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001cd77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_misclassified.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec38d7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 2986\n",
    "orig_index = df_misclassified[df_misclassified['index'] == index].index.values[0]\n",
    "movie_id = df.loc[index]['movie_id']\n",
    "print(df.loc[index])\n",
    "print(X_test.loc[index])\n",
    "print(orig_index)\n",
    "print(movie_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fab194",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta[df_meta['movie_id'] == movie_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cc507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158bea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf.get_feature_names_out()\n",
    "explainer = shap.Explainer(model, X_train, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47151c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029930ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shapexplainer = shap.Explainer(forest, X_train, feature_names=feature_names)\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3100ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_exp = explainer(X_test)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a4f8bcb",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "expected_value = explainer.expected_value\n",
    "if isinstance(expected_value, list):\n",
    "    expected_value = expected_value[1]\n",
    "    \n",
    "shap.decision_plot(explainer.expected_value[0], shap_values[0], feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76d7a36",
   "metadata": {},
   "source": [
    "#### Global Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa25e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # explainer for tree-based models\n",
    "    shap.plots.bar(shap_values_exp[:,:,1], max_display=20)\n",
    "except IndexError:\n",
    "    # falling back to standard explainer\n",
    "    print('Falling back to standard explainer')\n",
    "    shap.plots.bar(shap_values_exp, max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3286735",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test, plot_type='bar',feature_names=feature_names, max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6f1495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the first argument from 0 to 1 to see the chart from other angle\n",
    "\n",
    "try:\n",
    "    shap.summary_plot(shap_values[0], X_test, plot_type='violin',feature_names=feature_names, max_display=20)\n",
    "except AssertionError:\n",
    "    print('Falling back to standard explainer')\n",
    "    shap.summary_plot(shap_values, X_test, plot_type='violin',feature_names=feature_names, max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38562f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the first argument from 1 to 0 to see the chart from other angle\n",
    "\n",
    "try:\n",
    "    shap.summary_plot(shap_values[1], X_test, plot_type='dot',feature_names=feature_names, max_display=20)\n",
    "except AssertionError:\n",
    "    print('Falling back to standard explainer')\n",
    "    shap.summary_plot(shap_values, X_test, plot_type='violin',feature_names=feature_names, max_display=20)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c2583ec2",
   "metadata": {},
   "source": [
    "# TODO: not working for non tree based models\n",
    "\n",
    "#shap.plots.scatter(shap_values_exp[:,feature_names.tolist().index(\"machine\"),1])\n",
    "shap.plots.scatter(shap_values_exp[:,feature_names.tolist().index(\"machine\")], X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b29d9ab",
   "metadata": {},
   "source": [
    "#### Local Explainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee68f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    shap.plots.waterfall(shap_values_exp[orig_index,:,1], max_display=20)\n",
    "except IndexError:\n",
    "    print('Falling back')\n",
    "    shap.plots.waterfall(shap_values_exp[orig_index], max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e7a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    shap.plots.bar(shap_values_exp[orig_index,:,1], max_display=20)\n",
    "except IndexError:\n",
    "    print('Falling back')\n",
    "    shap.plots.bar(shap_values_exp[orig_index], max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7fda5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: to understand this plot\n",
    "try:\n",
    "    shap.dependence_plot(feature_names.tolist().index('like'), shap_values[1], X_test)\n",
    "except TypeError:\n",
    "    shap.dependence_plot(feature_names.tolist().index('like'), shap_values, X_test)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9498f431",
   "metadata": {},
   "source": [
    "shap.plots.bar(shap_values[index,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aa9d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    data = pd.Series(x)\n",
    "    return model.predict_proba(tfidf.transform(data))\n",
    "\n",
    "masker = shap.maskers.Text(r\"\\W\")\n",
    "corpus = [df.loc[index]['cleaned_plot']]\n",
    "single_explainer = shap.Explainer(predict, masker, output_names=['Non Sci-Fi','Sci-Fi'])\n",
    "single_shap_values = single_explainer(corpus)\n",
    "shap.plots.text(single_shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7345453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "choosen_instance = X_test.loc[index]\n",
    "shap_values2 = explainer.shap_values(choosen_instance)\n",
    "try:\n",
    "    plot = shap.force_plot(explainer.expected_value[1], shap_values2[1], choosen_instance)\n",
    "except IndexError:\n",
    "    plot = shap.force_plot(explainer.expected_value, shap_values2, choosen_instance)\n",
    "# the code block did not display the chart in the try-catch so I had to explicitly make the plot to be shown with this last line\n",
    "plot"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22673008",
   "metadata": {},
   "source": [
    "shap.plots.text(shap_values)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b531cc1",
   "metadata": {},
   "source": [
    "model_features = list(X.columns)\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=37)\n",
    "\n",
    "#print (str(len(X.index)))\n",
    "\n",
    "for model in [forest]:\n",
    "       \n",
    "    auc_buf = []\n",
    "\n",
    "    for fold_, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "        y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        #conf_mat = confusion_matrix(y_test, predictions)\n",
    "        #print(conf_mat)\n",
    "        #print(str(len(X_train.index)))\n",
    "        #print(str(len(X_test.index)))\n",
    "        \n",
    "        auc_buf.append(model.score(X_test, y_test))\n",
    "        \n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = model_features\n",
    "        fold_importance_df[\"importance\"] = model.feature_importances_\n",
    "        fold_importance_df[\"fold\"] = fold_ + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        \n",
    "    print ('%s mean accuracy score: %f' % (model.__class__.__name__, np.mean(auc_buf))) \n",
    "    \n",
    "cols = (feature_importance_df[[\"feature\", \"importance\"]]\n",
    "        .groupby(\"feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:100].index)\n",
    "\n",
    "best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(9,18))\n",
    "sns.barplot(x=\"importance\",y=\"feature\",data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "plt.title('Most Important Features (avg over folds)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e44f644",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a441cf3c",
   "metadata": {},
   "source": [
    "### Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fa497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('movie_genre_prediction/cleaned_plots.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e675afd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=[gensim_utils.simple_preprocess(x) for x in df['cleaned_plot'].tolist()]\n",
    "\n",
    "vector_size = 300\n",
    "window_size = 10\n",
    "min_count = 10\n",
    "\n",
    "w2v_model = gensim.models.Word2Vec(sentences,\n",
    "                                   vector_size=vector_size,\n",
    "                                   window=window_size,\n",
    "                                   min_count=min_count)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10c16805",
   "metadata": {},
   "source": [
    "w2v_model.wv.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aaa9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar('deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed43d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7ee93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de3ecd5e",
   "metadata": {},
   "source": [
    "w2v_model.wv.most_similar(positive=[\"ship\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f72949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the documents into words\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "df['tok_plot'] = df['cleaned_plot'].str.lower().apply(word_tokenize)\n",
    "#df['tok_plot_bi'] = df['tok_plot'].apply(lambda x: [x[0] + ' ' + x[1] for x in list(nltk.bigrams(x))])\n",
    "#df['tok_plot_sum'] = df['tok_plot'] + df['tok_plot_bi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afc6236",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "303437b9",
   "metadata": {},
   "source": [
    "set(w2v_model.wv.index_to_key )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59b4a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the tokenized words into list of word vectors\n",
    "\n",
    "words = set(w2v_model.wv.index_to_key )\n",
    "df['vect_plot'] = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
    "                         for ls in df['tok_plot']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c277e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc36062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as the length of the document vary so does the length of word vector list\n",
    "# for machine learning we need same size word vector list\n",
    "# the word count of the tokens and the length of the word vector list is different because of the words that are not\n",
    "# in the vocabulary (e.g. too few occurences or being a stop word)\n",
    "\n",
    "for i, v in enumerate(df['vect_plot']):\n",
    "    print(len(df['tok_plot'].iloc[i]), len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1797d8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to have the same size vectors for all document we are generating the averaged document vectors\n",
    "# the result is a constant size word vector for all documents\n",
    "\n",
    "text_vect_avg = []\n",
    "for v in df['vect_plot']:\n",
    "    # v.size = length of word vector list * word vector size\n",
    "    if v.size:\n",
    "        text_vect_avg.append(v.mean(axis=0))\n",
    "        # axis=0 means it is averaging the values in the same position in all lists. This guarantees the same size.\n",
    "        # The number of lists are different for each documents but the length of the word vectors are the same.\n",
    "    else:\n",
    "        text_vect_avg.append(np.zeros(vector_size, dtype=float)) # the same vector size must be used here as for model training\n",
    "        \n",
    "        \n",
    "df['vect_plot_avg'] = text_vect_avg\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5502457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can see that the vector lengths are constant\n",
    "\n",
    "for i, v in enumerate(df['vect_plot_avg']):\n",
    "    print(len(df['tok_plot'].iloc[i]), len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89a28fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(text_vect_avg)\n",
    "df_train.columns = ['vec_avg_' + str(i+1) for i in range(0, df_train.shape[1])]\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2db67a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([df[['sci']], df_train], axis=1, sort=False)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ddfb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_df.drop(['sci'], axis=1,errors='ignore')\n",
    "y = final_df['sci']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9b4941",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=1000)\n",
    "knn = KNeighborsClassifier()\n",
    "d_tree = DecisionTreeClassifier()\n",
    "forest = RandomForestClassifier()\n",
    "svm = LinearSVC()\n",
    "\n",
    "for model in [lr,knn,d_tree,forest,svm]:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print ('%s accuracy score: %f' % (model.__class__.__name__, model.score(X_test, y_test)))\n",
    "    #print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb300b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3b1d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_closestwords_tsnescatterplot(w2v_model.wv, 300, ['love'], 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63576e30",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f8d43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('movie_genre_prediction/cleaned_plots.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9776a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tok_plot'] = df['cleaned_plot'].str.lower().apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f5952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file = datapath(r'e:\\python\\nlp\\glove.6B.300d.txt')\n",
    "tmp_file = get_tmpfile('test_word2vec.txt')\n",
    "\n",
    "_ = glove2word2vec(glove_file, tmp_file)\n",
    "w2v_model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6256ab33",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.most_similar('deep', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3d0b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(w2v_model.index_to_key)\n",
    "words = words - set(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f5d5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vect_plot'] = np.array([np.array([w2v_model[i] for i in ls if i in words])\n",
    "                         for ls in df['tok_plot']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35534f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee98b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as the length of the document vary so does the length of word vector list\n",
    "# for machine learning we need same size word vector list\n",
    "# the word count of the tokens and the length of the word vector list is different because of the words that are not\n",
    "# in the vocabulary (e.g. too few occurences or being a stop word)\n",
    "\n",
    "for i, v in enumerate(df['vect_plot']):\n",
    "    print(len(df['tok_plot'].iloc[i]), len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bbd079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to have the same size vectors for all document we are generating the averaged document vectors\n",
    "# the result is a constant size word vector for all documents\n",
    "\n",
    "text_vect_avg = []\n",
    "for v in df['vect_plot']:\n",
    "    # v.size = length of word vector list * word vector size\n",
    "    if v.size:\n",
    "        text_vect_avg.append(v.mean(axis=0))\n",
    "        # axis=0 means it is averaging the values in the same position in all lists. This guarantees the same size.\n",
    "        # The number of lists are different for each documents but the length of the word vectors are the same.\n",
    "    else:\n",
    "        text_vect_avg.append(np.zeros(vector_size, dtype=float)) # the same vector size must be used here as for model training\n",
    "        \n",
    "        \n",
    "df['vect_plot_avg'] = text_vect_avg\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b77fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can see that the vector lengths are constant\n",
    "\n",
    "for i, v in enumerate(df['vect_plot_avg']):\n",
    "    print(len(df['tok_plot'].iloc[i]), len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e388c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(text_vect_avg)\n",
    "df_train.columns = ['vec_avg_' + str(i+1) for i in range(0, df_train.shape[1])]\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2daed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([df[['sci']], df_train], axis=1, sort=False)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b547f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_df.drop(['sci'], axis=1,errors='ignore')\n",
    "y = final_df['sci']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5149311",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=1000)\n",
    "knn = KNeighborsClassifier()\n",
    "d_tree = DecisionTreeClassifier()\n",
    "forest = RandomForestClassifier()\n",
    "svm = LinearSVC()\n",
    "\n",
    "for model in [lr,knn,d_tree,forest,svm]:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print ('%s accuracy score: %f' % (model.__class__.__name__, model.score(X_test, y_test)))\n",
    "    #print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f50f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec1c90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_closestwords_tsnescatterplot(w2v_model, 300, ['alien'], 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0884ee7",
   "metadata": {},
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be98d8ea",
   "metadata": {},
   "source": [
    "#### in standalone mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49da4449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasttext is a subword based model\n",
    "\n",
    "df = pd.read_csv('movie_genre_prediction/cleaned_plots.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b17504a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_target_label(row):\n",
    "    sci = row['sci']\n",
    "    text = row['cleaned_plot']\n",
    "    \n",
    "    return '__label__' + str(sci) + ' ' + text\n",
    "    \n",
    "# fasttext expects to have the labels concatenated with the original text in the format of __label__<label_value><text>\n",
    "df['cleaned_plot'] = df.apply(concat_target_label, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abeef8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719c833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df[['sci','cleaned_plot']], test_size=0.2, random_state=31, \n",
    "                                     shuffle=True, stratify=df['sci'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef63bd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e7fd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the training and test set has to be in a separate file for fasttext\n",
    "\n",
    "df_train.to_csv('train.txt', \n",
    "    index = False, \n",
    "    sep = ' ',\n",
    "    header = None, \n",
    "    quoting = csv.QUOTE_NONE, \n",
    "    quotechar = \"\", \n",
    "    escapechar = \" \")\n",
    "\n",
    "df_test.to_csv('test.txt', \n",
    "    index = False, \n",
    "    sep = ' ',\n",
    "    header = None, \n",
    "    quoting = csv.QUOTE_NONE, \n",
    "    quotechar = \"\", \n",
    "    escapechar = \" \")\n",
    "\n",
    "\n",
    "# Training the fastText classifier.\n",
    "# in this case the unigrams resulted in a stronger model than longer n-grams\n",
    "model = fasttext.train_supervised('train.txt', wordNgrams = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fe412e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test('test.txt')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d71485f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_nearest_neighbors('alien', k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369baaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(\"Earth's future has been riddled by disasters, famines, and droughts. There is only one way to ensure mankind's survival: Interstellar travel. A newly discovered wormhole in the far reaches of our solar system allows a team of astronauts to go where no man has gone before, a planet that may have the right environment to sustain human life\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c603753f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the data\n",
    "df_test[\"predicted\"] = df_test[\"cleaned_plot\"].apply(lambda x: int(model.predict(x)[0][0].replace('__label__','')))\n",
    "\n",
    "# Create the confusion matrix\n",
    "confusion_matrix(df_test[\"sci\"], df_test[\"predicted\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec6063",
   "metadata": {},
   "source": [
    "#### as a gensim model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462f374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('movie_genre_prediction/cleaned_plots.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d139011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a066614",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = FastText(vector_size=300, window=10, min_count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694d9cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=[gensim_utils.simple_preprocess(x) for x in df['cleaned_plot'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490aa3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model.build_vocab(corpus_iterable=sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c237aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model.train(corpus_iterable=sentences, total_examples=len(sentences), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ada86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model.wv.most_similar('alien')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3958649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the documents into words\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "df['tok_plot'] = df['cleaned_plot'].str.lower().apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2c7f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c52d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(ft_model.wv.index_to_key )\n",
    "df['vect_plot'] = np.array([np.array([ft_model.wv[i] for i in ls if i in words])\n",
    "                         for ls in df['tok_plot']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b94282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5560f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as the length of the document vary so does the length of word vector list\n",
    "# for machine learning we need same size word vector list\n",
    "# the word count of the tokens and the length of the word vector list is different because of the words that are not\n",
    "# in the vocabulary (e.g. too few occurences or being a stop word)\n",
    "\n",
    "for i, v in enumerate(df['vect_plot']):\n",
    "    print(len(df['tok_plot'].iloc[i]), len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff35637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vect_avg = []\n",
    "for v in df['vect_plot']:\n",
    "    # v.size = length of word vector list * word vector size\n",
    "    if v.size:\n",
    "        text_vect_avg.append(v.mean(axis=0))\n",
    "        # axis=0 means it is averaging the values in the same position in all lists. This guarantees the same size.\n",
    "        # The number of lists are different for each documents but the length of the word vectors are the same.\n",
    "    else:\n",
    "        text_vect_avg.append(np.zeros(vector_size, dtype=float)) # the same vector size must be used here as for model training\n",
    "        \n",
    "        \n",
    "df['vect_plot_avg'] = text_vect_avg\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58f1866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can see that the vector lengths are constant\n",
    "\n",
    "for i, v in enumerate(df['vect_plot_avg']):\n",
    "    print(len(df['tok_plot'].iloc[i]), len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b03c96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(text_vect_avg)\n",
    "df_train.columns = ['vec_avg_' + str(i+1) for i in range(0, df_train.shape[1])]\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a2f477",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([df[['sci']], df_train], axis=1, sort=False)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d18074",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_df.drop(['sci'], axis=1,errors='ignore')\n",
    "y = final_df['sci']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631dd8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=1000)\n",
    "knn = KNeighborsClassifier()\n",
    "d_tree = DecisionTreeClassifier()\n",
    "forest = RandomForestClassifier()\n",
    "svm = LinearSVC()\n",
    "\n",
    "for model in [lr,knn,d_tree,forest,svm]:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print ('%s accuracy score: %f' % (model.__class__.__name__, model.score(X_test, y_test)))\n",
    "    #print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2df251",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be7e034",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_closestwords_tsnescatterplot(ft_model.wv, 300, ['alien'], 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8a1ff1",
   "metadata": {},
   "source": [
    "### Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaad4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, TFAutoModel, TFAutoModelForSequenceClassification, DataCollatorWithPadding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b522c22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('movie_genre_prediction/cleaned_plots_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e83172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=31, shuffle=True, stratify=df['sci'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3733ade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test, df_val = train_test_split(df_test, test_size=0.1, random_state=31, shuffle=True, stratify=df_test['sci'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "29469b49",
   "metadata": {},
   "source": [
    "dataset = load_dataset(\"csv\", data_files=\"movie_genre_prediction/cleaned_plots.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfc62c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_pandas(df_train, split=\"train\")\n",
    "test_ds = Dataset.from_pandas(df_test, split=\"test\")\n",
    "val_ds = Dataset.from_pandas(df_val, split=\"validation\")\n",
    "\n",
    "dataset_dict = DatasetDict({\"train\":train_ds,\"test\":test_ds,\"val\":val_ds})\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd53e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa70eec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f06cc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7861ae7a",
   "metadata": {},
   "source": [
    "text = \"Tokenizing text is a core task of NLP.\"\n",
    "encoded_text = tokenizer(text)\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "be8c3725",
   "metadata": {},
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0a83ddc",
   "metadata": {},
   "source": [
    "print(tokenizer.convert_tokens_to_string(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6e8c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"cleaned_plot\"], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcf8e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_datasets = dataset_dict.map(tokenize, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e02454",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_datasets"
   ]
  },
  {
   "cell_type": "raw",
   "id": "41bd7272",
   "metadata": {},
   "source": [
    "print(encoded_datasets['train'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7b024a",
   "metadata": {},
   "source": [
    "#### Transformer as feature extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ec1879",
   "metadata": {},
   "source": [
    "Transformers can extract features from text similarly to word2vec. The extracted features are the hidden states of the neural network under the hood of the transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3171db2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model = TFAutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e02b6aad",
   "metadata": {},
   "source": [
    "text = \"this is a test\"\n",
    "inputs = tokenizer(text, return_tensors='tf')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ebf1de75",
   "metadata": {},
   "source": [
    "outputs = tf_model(inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ae61292",
   "metadata": {},
   "source": [
    "outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56879b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_datasets.reset_format()\n",
    "\n",
    "def extract_hidden_states(batch):\n",
    "    # First convert text to tokens\n",
    "    inputs = tokenizer(batch[\"cleaned_plot\"], padding=True, truncation=True, return_tensors='tf')\n",
    "    # Extract last hidden states\n",
    "    outputs = tf_model(inputs)\n",
    "    return {\"hidden_state\": outputs.last_hidden_state[:,0].numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c32c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# larger batch sizes cause out of resource errors\n",
    "datasets_hidden = encoded_datasets.map(extract_hidden_states, batched=True, batch_size=16)\n",
    "datasets_hidden"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f1ea66b4",
   "metadata": {},
   "source": [
    "X_train[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aa461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(datasets_hidden[\"train\"][\"hidden_state\"]) \n",
    "X_valid = np.array(datasets_hidden[\"test\"][\"hidden_state\"]) \n",
    "y_train = np.array(datasets_hidden[\"train\"][\"sci\"]) \n",
    "y_valid = np.array(datasets_hidden[\"test\"][\"sci\"]) \n",
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4c4dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression(max_iter=3000) \n",
    "lr_clf.fit(X_train, y_train) \n",
    "lr_clf.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f636d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = lr_clf.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d79762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(y_true, y_pred, figsize=(7, 5)):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum.astype(float) * 100\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    nrows, ncols = cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
    "            elif c == 0:\n",
    "                annot[i, j] = ''\n",
    "            else:\n",
    "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
    "    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n",
    "    cm.index.name = 'Actual'\n",
    "    cm.columns.name = 'Predicted'\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)\n",
    "    \n",
    "df_eval = pd.DataFrame({'y_true': y_valid, 'y_preds': y_preds})\n",
    "plot_cm(df_eval['y_true'], df_eval['y_preds'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e513699b",
   "metadata": {},
   "source": [
    "#### Fine tuning transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c54fa85",
   "metadata": {},
   "source": [
    "An other way transformers can classify is that instead of using the hidden states as features we train them as part of the\n",
    "classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa1b454",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model = (TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190b0a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_columns = tokenizer.model_input_names\n",
    "# Define a batch size\n",
    "batch_size = 8\n",
    "# collator is here to support batching\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "tf_train_dataset = encoded_datasets[\"train\"].to_tf_dataset(columns=tokenizer_columns,label_cols=[\"sci\"], \n",
    "                                                           shuffle=True,batch_size=batch_size,collate_fn=data_collator)\n",
    "tf_test_dataset = encoded_datasets[\"test\"].to_tf_dataset(columns=tokenizer_columns, label_cols=[\"sci\"], \n",
    "                                                         shuffle=False,batch_size=batch_size,\n",
    "                                                         collate_fn=data_collator)\n",
    "tf_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b0a777",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "                 metrics=tf.metrics.SparseCategoricalAccuracy())\n",
    "tf_model.fit(tf_train_dataset, validation_data=tf_test_dataset, epochs=2)\n",
    "loss, eval_accuracy = tf_model.evaluate(tf_test_dataset)\n",
    "print(\"Loss: {}\\t Test Accuracy: {}\".format(loss, eval_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d30cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_logits = tf_model.predict(tf_test_dataset).logits\n",
    "pred_labels = np.argmax(output_logits, axis=-1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "16a0554f",
   "metadata": {},
   "source": [
    "output_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4629c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_datasets[\"test\"] = encoded_datasets[\"test\"].add_column(\"predicted_label\", pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fd3513",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5487f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_datasets.set_format(\"pandas\") \n",
    "cols = [\"cleaned_plot\", \"sci\", \"predicted_label\"]\n",
    "df_test = encoded_datasets[\"test\"][:][cols] \n",
    "df_test.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d942d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cm(df_test.sci, df_test.predicted_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
