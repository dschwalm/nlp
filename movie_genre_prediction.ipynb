{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a35f62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import text_normalizer as tn\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import tree\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import lightgbm as lgbm\n",
    "\n",
    "import shap\n",
    "\n",
    "from sklearn.model_selection import KFold \n",
    "\n",
    "pd.set_option('display.max_rows',1000)\n",
    "pd.set_option('display.max_columns',1000)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(37)\n",
    "random.seed(17)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722b1bc7",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fae264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MovieSummaries dataset. Source: http://www.cs.cmu.edu/~ark/personas/\n",
    "df_meta = pd.read_csv('movie_genre_prediction/movie.metadata.tsv', sep='\\t')\n",
    "df_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11559ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = pd.read_csv('movie_genre_prediction/plot_summaries.txt', sep='\\t')\n",
    "#df_plot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80f8cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df_plot, df_meta,on='movie_id',how='left')\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2222a34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['x1','title','date','x2','x3','lang','country'],axis=1,inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5cfd6415",
   "metadata": {},
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ed6e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['plot'] = df['plot'].astype(str)\n",
    "df['tags'] = df['tags'].astype(str)\n",
    "df['tags'] = df['tags'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6987cae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sci'] = ''\n",
    "df['sci'] = df['tags'].apply(lambda x : 1 if 'science fiction' in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0590763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sci = df[df['sci'] == 1]\n",
    "df_sci.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18015649",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_sci = df[df['sci'] == 0][:2500]\n",
    "df_non_sci.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_sci,df_non_sci])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854dad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['sci'].value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "987f9b5b",
   "metadata": {},
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5181fdb2",
   "metadata": {},
   "source": [
    "# document_string = \"\"\"In the final 23 days of L's life, he meets one final case involving a bioterrorist group that aims to wipe out much of humanity with a virus. The virus has an infection rate that has one hundred times the infection rate of the Ebola virus. He takes a boy he names Near, the sole survivor of its use in a village in Thailand, and an elementary school student named Maki Nikaido under his wing. Dr. Nikaido later received a sample of the deadly virus which destroyed that village in Thailand. His assistant, Dr. Kimiko Kujo, reveals herself to be the leader of the organization that created the virus. Dr. Nikaido, who has created an antidote to that virus, refuses to give it her. He destroys the antidote and injects himself with the virus. She later kills him, and she is convinced that his daughter Maki has the antidote formula. Under the pursuit of Dr. Kimiko Kujo and her assistants, Maki runs and escapes. She eventually found L's headquarters. However, the group manages to track Maki down, forcing L, accompanied by Maki and Near, to run away with a high-tech crepe truck. They also received the help of FBI agent Hideaki Suruga during the escape. They escape to Nikaido's research partner's lab, because they needed his help to recreate the antidote. Using Near, L manages to acquire the antidote just as the terrorists are about to take an infected Maki to the US to spread the virus. L stops the plane and gives all the infected passengers, including the terrorist, the antidote. Maki then tries to kill Kujo for revenge, but L stops her, she goes to the hospital and wakes up with her stuffed bear next to her and a recording from L telling her to have a good day tomorrow. The film concludes with L leaving Near and giving him Near's \"real name\". \"\"\"\n",
    "    \n",
    "from spacy.tokens import Span\n",
    "from spacy import Language\n",
    "import dateparser\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "@Language.component(\"my_pipeline_component\")\n",
    "def expand_person_entities(doc):\n",
    "    new_ents = []\n",
    "    for ent in doc.ents:\n",
    "        # Only check for title if it's a person and not the first token\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            if ent.start != 0:\n",
    "                # if person preceded by title, include title in entity\n",
    "                prev_token = doc[ent.start - 1]\n",
    "                if prev_token.text in (\"Dr\", \"Dr.\", \"Mr\", \"Mr.\", \"Ms\", \"Ms.\"):\n",
    "                    new_ent = Span(doc, ent.start - 1, ent.end, label=ent.label)\n",
    "                    new_ents.append(new_ent)\n",
    "                else:\n",
    "                    # if entity can be parsed as a date, it's not a person\n",
    "                    if dateparser.parse(ent.text) is None:\n",
    "                        new_ents.append(ent) \n",
    "        else:\n",
    "            new_ents.append(ent)\n",
    "    doc.ents = new_ents\n",
    "    return doc\n",
    "\n",
    "\n",
    "nlp.add_pipe(\"my_pipeline_component\", after='ner')\n",
    "\n",
    "for index in tqdm(df_train.index):\n",
    "    person_names = set()\n",
    "    doc = nlp(df_train.at[index, 'plot'])\n",
    "\n",
    "    for person_name, _ in [(ent.text, ent.label_) for ent in doc.ents if ent.label_=='PERSON']:\n",
    "        person_names.add(person_name)\n",
    "        \n",
    "    for name in person_names:\n",
    "        #df_train.iloc[index]['plot'] = df_train.iloc[index]['plot'].replace(name,'',df_train.iloc[index]['plot'])\n",
    "        df_train.at[index, 'plot'] = df_train.at[index, 'plot'].replace(name,'')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d233cd9e",
   "metadata": {},
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc53d034",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['cleaned_plot'] = tn.normalize_corpus(corpus=df_train['plot'],stopwords=stopwords)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba5427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(['plot','tags'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d5f5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('cleaned_plots.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75b3102",
   "metadata": {},
   "source": [
    "### TF IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6fffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(''movie_genre_prediction/cleaned_plots.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f594ffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords =  stopwords + ['los', 'must', 'may', 'could','jim','would','without','also','thus','however','ben']\n",
    "max_features = 700\n",
    "min_df = 10\n",
    "max_df = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315745a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=31, shuffle=True, stratify=df['sci'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e9b2eb44",
   "metadata": {},
   "source": [
    "count_vect = CountVectorizer(stop_words=stopwords,analyzer='word',token_pattern='[^\\W\\d_]{2,}',\\\n",
    "                             ngram_range=(1,3), max_features=max_features, min_df=min_df,max_df=max_df,strip_accents='unicode')\n",
    "X_train_counts = count_vect.fit_transform(texts)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "weights = np.asarray(X_train_tfidf.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': count_vect.get_feature_names(), 'weight': weights})\n",
    "weights_df = weights_df.sort_values(ascending=False, by=['weight'])\n",
    "weights_df.head(100)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d664a2e",
   "metadata": {},
   "source": [
    "weights_df.tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10ffc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=max_features, min_df=min_df,max_df=max_df,stop_words=stopwords,analyzer='word',\\\n",
    "                            token_pattern='[^\\W\\d_]{2,}', ngram_range=(1,3),strip_accents='unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3f7b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = df_train['cleaned_plot'].tolist()\n",
    "\n",
    "tfidf.fit(train_texts)\n",
    "\n",
    "tfidf_train = np.array(tfidf.transform(df_train['cleaned_plot']).todense(), dtype=np.float16)\n",
    "\n",
    "tfidf_feature_names = { v:k for k,v in tfidf.vocabulary_.items() }\n",
    "\n",
    "for i in range(max_features):\n",
    "    df_train['tfidf_' + tfidf_feature_names[i]] = tfidf_train[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84350236",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test = np.array(tfidf.transform(df_test['cleaned_plot']).todense(), dtype=np.float16)\n",
    "\n",
    "for i in range(max_features):\n",
    "    df_test['tfidf_' + tfidf_feature_names[i]] = tfidf_test[:, i]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b5a07d0",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bf7b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(['movie_id','sci','cleaned_plot'], axis=1,errors='ignore')\n",
    "y_train = df_train['sci']\n",
    "X_test = df_test.drop(['movie_id','sci','cleaned_plot'], axis=1,errors='ignore')\n",
    "y_test = df_test['sci']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4911bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "knn = KNeighborsClassifier()\n",
    "d_tree = DecisionTreeClassifier()\n",
    "forest = RandomForestClassifier()\n",
    "svm = LinearSVC()\n",
    "lgm = lgbm.LGBMClassifier()\n",
    "\n",
    "for model in [svm, knn,d_tree,lr,lgm,forest]:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print ('%s accuracy score: %f' % (model.__class__.__name__, model.score(X_test, y_test)))\n",
    "    #print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2b9007ad",
   "metadata": {},
   "source": [
    "np.where(y_test != y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf232e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = X_test.copy()\n",
    "df_out['truth'] = y_test\n",
    "df_out.reset_index(inplace=True)\n",
    "df_out['predicted'] = y_pred\n",
    "df_misclassified = df_out[df_out['truth'] != df_out['predicted']][['index','truth','predicted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001cd77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_misclassified.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec38d7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1166\n",
    "orig_index = df_misclassified[df_misclassified['index'] == index].index.values[0]\n",
    "movie_id = df.loc[index]['movie_id']\n",
    "print(df.loc[index])\n",
    "print(X_test.loc[index])\n",
    "print(orig_index)\n",
    "print(movie_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fab194",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta[df_meta['movie_id'] == movie_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14da6ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158bea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf.get_feature_names_out()\n",
    "explainer = shap.Explainer(model, X_train, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47151c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029930ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shapexplainer = shap.Explainer(forest, X_train, feature_names=feature_names)\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3100ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_exp = explainer(X_test)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a4f8bcb",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "expected_value = explainer.expected_value\n",
    "if isinstance(expected_value, list):\n",
    "    expected_value = expected_value[1]\n",
    "    \n",
    "shap.decision_plot(explainer.expected_value[0], shap_values[0], feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76d7a36",
   "metadata": {},
   "source": [
    "#### Global Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa25e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # explainer for tree-based models\n",
    "    shap.plots.bar(shap_values_exp[:,:,1], max_display=20)\n",
    "except IndexError:\n",
    "    # falling back to standard explainer\n",
    "    print('Falling back to standard explainer')\n",
    "    shap.plots.bar(shap_values_exp, max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3286735",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test, plot_type='bar',feature_names=feature_names, max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6f1495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the first argument from 0 to 1 to see the chart from other angle\n",
    "\n",
    "try:\n",
    "    shap.summary_plot(shap_values[0], X_test, plot_type='violin',feature_names=feature_names, max_display=20)\n",
    "except AssertionError:\n",
    "    print('Falling back to standard explainer')\n",
    "    shap.summary_plot(shap_values, X_test, plot_type='violin',feature_names=feature_names, max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38562f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the first argument from 1 to 0 to see the chart from other angle\n",
    "\n",
    "try:\n",
    "    shap.summary_plot(shap_values[1], X_test, plot_type='dot',feature_names=feature_names, max_display=20)\n",
    "except AssertionError:\n",
    "    print('Falling back to standard explainer')\n",
    "    shap.summary_plot(shap_values, X_test, plot_type='violin',feature_names=feature_names, max_display=20)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5177a64f",
   "metadata": {},
   "source": [
    "# TODO: not working for non tree based models\n",
    "\n",
    "#shap.plots.scatter(shap_values_exp[:,feature_names.tolist().index(\"machine\"),1])\n",
    "shap.plots.scatter(shap_values_exp[:,feature_names.tolist().index(\"machine\")], X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b29d9ab",
   "metadata": {},
   "source": [
    "#### Local Explainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee68f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    shap.plots.waterfall(shap_values_exp[orig_index,:,1], max_display=20)\n",
    "except IndexError:\n",
    "    print('Falling back')\n",
    "    shap.plots.waterfall(shap_values_exp[orig_index], max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e7a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    shap.plots.bar(shap_values_exp[orig_index,:,1], max_display=20)\n",
    "except IndexError:\n",
    "    print('Falling back')\n",
    "    shap.plots.bar(shap_values_exp[orig_index], max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7fda5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: to understand this plot\n",
    "try:\n",
    "    shap.dependence_plot(feature_names.tolist().index('like'), shap_values[1], X_test)\n",
    "except TypeError:\n",
    "    shap.dependence_plot(feature_names.tolist().index('like'), shap_values, X_test)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9498f431",
   "metadata": {},
   "source": [
    "shap.plots.bar(shap_values[index,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aa9d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    data = pd.Series(x)\n",
    "    return model.predict_proba(tfidf.transform(data))\n",
    "\n",
    "masker = shap.maskers.Text(r\"\\W\")\n",
    "corpus = [df.loc[index]['cleaned_plot']]\n",
    "single_explainer = shap.Explainer(predict, masker, output_names=['Non Sci-Fi','Sci-Fi'])\n",
    "single_shap_values = single_explainer(corpus)\n",
    "shap.plots.text(single_shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7345453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "choosen_instance = X_test.loc[index]\n",
    "shap_values2 = explainer.shap_values(choosen_instance)\n",
    "try:\n",
    "    plot = shap.force_plot(explainer.expected_value[1], shap_values2[1], choosen_instance)\n",
    "except IndexError:\n",
    "    plot = shap.force_plot(explainer.expected_value, shap_values2, choosen_instance)\n",
    "# the code block did not display the chart in the try-catch so I had to explicitly make the plot to be shown with this last line\n",
    "plot"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22673008",
   "metadata": {},
   "source": [
    "shap.plots.text(shap_values)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b531cc1",
   "metadata": {},
   "source": [
    "model_features = list(X.columns)\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=37)\n",
    "\n",
    "#print (str(len(X.index)))\n",
    "\n",
    "for model in [forest]:\n",
    "       \n",
    "    auc_buf = []\n",
    "\n",
    "    for fold_, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "        y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        #conf_mat = confusion_matrix(y_test, predictions)\n",
    "        #print(conf_mat)\n",
    "        #print(str(len(X_train.index)))\n",
    "        #print(str(len(X_test.index)))\n",
    "        \n",
    "        auc_buf.append(model.score(X_test, y_test))\n",
    "        \n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = model_features\n",
    "        fold_importance_df[\"importance\"] = model.feature_importances_\n",
    "        fold_importance_df[\"fold\"] = fold_ + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        \n",
    "    print ('%s mean accuracy score: %f' % (model.__class__.__name__, np.mean(auc_buf))) \n",
    "    \n",
    "cols = (feature_importance_df[[\"feature\", \"importance\"]]\n",
    "        .groupby(\"feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:100].index)\n",
    "\n",
    "best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(9,18))\n",
    "sns.barplot(x=\"importance\",y=\"feature\",data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "plt.title('Most Important Features (avg over folds)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e44f644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a441cf3c",
   "metadata": {},
   "source": [
    "### Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fa497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_plots.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e675afd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import utils as gensim_utils\n",
    "\n",
    "sentences=[gensim_utils.simple_preprocess(x) for x in df['cleaned_plot'].tolist()]\n",
    "\n",
    "vector_size = 300\n",
    "window_size = 10\n",
    "min_count = 10\n",
    "\n",
    "w2v_model = gensim.models.Word2Vec(sentences,\n",
    "                                   vector_size=vector_size,\n",
    "                                   window=window_size,\n",
    "                                   min_count=min_count)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10c16805",
   "metadata": {},
   "source": [
    "w2v_model.wv.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aaa9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar('deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed43d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7ee93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de3ecd5e",
   "metadata": {},
   "source": [
    "w2v_model.wv.most_similar(positive=[\"ship\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f72949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the documents into words\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "df['tok_plot'] = df['cleaned_plot'].str.lower().apply(word_tokenize)\n",
    "#df['tok_plot_bi'] = df['tok_plot'].apply(lambda x: [x[0] + ' ' + x[1] for x in list(nltk.bigrams(x))])\n",
    "#df['tok_plot_sum'] = df['tok_plot'] + df['tok_plot_bi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afc6236",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "303437b9",
   "metadata": {},
   "source": [
    "set(w2v_model.wv.index_to_key )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59b4a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the tokenized words into list of word vectors\n",
    "\n",
    "words = set(w2v_model.wv.index_to_key )\n",
    "df['vect_plot'] = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
    "                         for ls in df['tok_plot']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c277e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc36062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as the length of the document vary so does the length of word vector list\n",
    "# for machine learning we need same size word vector list\n",
    "\n",
    "for i, v in enumerate(df['vect_plot']):\n",
    "    print(len(df['tok_plot'].iloc[i]), len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1797d8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to have the same size vectors for all document we are generating the averaged document vectors\n",
    "# the result is a constant size word vector for all documents\n",
    "\n",
    "text_vect_avg = []\n",
    "for v in df['vect_plot']:\n",
    "    if v.size:\n",
    "        text_vect_avg.append(v.mean(axis=0))\n",
    "    else:\n",
    "        text_vect_avg.append(np.zeros(vector_size, dtype=float)) # the same vector size must be used here as for model training\n",
    "        \n",
    "        \n",
    "df['vect_plot_avg'] = text_vect_avg\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5502457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can see that the vector lengths are constant\n",
    "\n",
    "for i, v in enumerate(df['vect_plot_avg']):\n",
    "    print(len(df['tok_plot'].iloc[i]), len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89a28fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(text_vect_avg)\n",
    "df_train.columns = ['vec_avg_' + str(i+1) for i in range(0, df_train.shape[1])]\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2db67a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([df[['sci']], df_train], axis=1, sort=False)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ddfb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_df.drop(['sci'], axis=1,errors='ignore')\n",
    "y = final_df['sci']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9b4941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "knn = KNeighborsClassifier()\n",
    "d_tree = DecisionTreeClassifier()\n",
    "forest = RandomForestClassifier()\n",
    "svm = LinearSVC()\n",
    "\n",
    "for model in [lr,knn,d_tree,forest,svm]:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print ('%s accuracy score: %f' % (model.__class__.__name__, model.score(X_test, y_test)))\n",
    "    #print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb300b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
